<!DOCTYPE html>
 <html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><meta name="viewport" content="width=device-width, initial-scale=1.0 maximum-scale=1.0"/><title>nabilhassein.github.io | Against Black Inclusion in Facial Recognition</title><link rel="shortcut icon"/><style id="typography.js">      /*! normalize.css v4.1.1 | MIT License | github.com/necolas/normalize.css */html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}progress{vertical-align:baseline}[hidden],template{display:none}a{background-color:transparent;-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit;font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}button,input,optgroup,select,textarea{font:inherit;margin:0}optgroup{font-weight:700}button,input{overflow:visible}button,select{text-transform:none}[type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}      html {        box-sizing:border-box;        font-size:112.5%;        line-height:27px;        overflow-y:scroll;      }      *, *:before, *:after {        box-sizing:inherit;      }      body {        color:hsl(0,0%,20%);        font-family:georgia, serif;        font-weight:300;        word-wrap:break-word;      }      /* Make image responsive by default */      img {        max-width:100%;      }    h1,h2,h3,h4,h5,h6,hgroup,ul,ol,dl,dd,p,figure,pre,table,fieldset,blockquote,form,noscript,iframe,img,hr{margin:0;      margin-bottom:1.5rem;      padding:0;}blockquote{margin:1.5rem 3.75rem;}b,strong{font-weight:600}hr{background:hsl(0,0%,80%);      border:none;      height:1px;      margin-bottom:calc(1.5rem - 1px);}ol,ul{list-style-position:outside;      margin-left:1.5rem;}ul li,ol li{padding-left:0;}code,kbd,pre,samp{font-size:0.85rem;      line-height:1.5rem;}table{font-size:1rem;      line-height:2.25rem;      width:100%;}thead{text-align:left;}h1,h2,h3,h4,h5,h6{color:hsl(0,0%,20%);      font-family:"Avenir Next", "Helvetica Neue", "Segoe UI", Helvetica, Arial, sans-serif;      font-weight:600;}h1{font-size:1.2rem;        line-height:1.5rem;}h2{font-size:1.14653rem;        line-height:1.5rem;}h3{font-size:1.09545rem;        line-height:1.5rem;}h4{font-size:1.04664rem;        line-height:1.5rem;}h5{font-size:1rem;        line-height:1.5rem;}h6{font-size:0.95544rem;        line-height:1.5rem;}             </style><style>.warning{color:#9f6000;background-color:#feefb3;padding:18px}.warning:before{content:"\26A0";margin-right:9px}.markdown pre{display:block;background:#3f3f3f;color:#dcdcdc;overflow-y:hidden}.markdown pre code{background:none;border:none;border-radius:3px;display:inline-block;overflow:inherit;padding:1.58333rem;white-space:inherit;word-wrap:normal}code{border-radius:3px;white-space:pre;white-space:pre-wrap;white-space:pre-line;white-space:-pre-wrap;white-space:-o-pre-wrap;white-space:-moz-pre-wrap;white-space:-hp-pre-wrap;word-wrap:break-word;background:#e5e5e5;border:1px solid #ccc;display:inline;font-family:Inconsolata,monospace,serif;max-width:100%;overflow:auto;padding:0 .1625rem}.clojure .hljs-attribute,.css .hljs-class,.css .hljs-id,.hljs-keyword,.hljs-request,.hljs-status,.hljs-tag,.lisp .hljs-title,.nginx .hljs-title{color:#e3ceab}.django .hljs-filter .hljs-argument,.django .hljs-template_tag,.django .hljs-variable{color:#dcdcdc}.hljs-date,.hljs-number{color:#8cd0d3}.apache .hljs-sqbracket,.dos .hljs-envvar,.dos .hljs-stream,.hljs-variable{color:#efdcbc}.diff .hljs-change,.dos .hljs-flow,.hljs-literal,.python .exception,.python .hljs-built_in,.tex .hljs-special{color:#efefaf}.diff .hljs-chunk,.hljs-subst{color:#8f8f8f}.apache .hljs-tag,.diff .hljs-header,.dos .hljs-keyword,.haskell .hljs-type,.hljs-prompt,.hljs-title,.nginx .hljs-built_in,.python .hljs-decorator,.ruby .hljs-class .hljs-parent,.tex .hljs-command{color:#efef8f}.dos .hljs-winutils,.ruby .hljs-string,.ruby .hljs-symbol,.ruby .hljs-symbol .hljs-string{color:#dca3a3}.apache .hljs-cbracket,.coffeescript .hljs-attribute,.css .hljs-rules .hljs-value,.diff .hljs-deletion,.hljs-attr_selector,.hljs-built_in,.hljs-javadoc,.hljs-pragma,.hljs-preprocessor,.hljs-pseudo,.hljs-string,.hljs-tag .hljs-value,.smalltalk .hljs-array,.smalltalk .hljs-class,.smalltalk .hljs-localvars,.sql .hljs-aggregate,.tex .hljs-formula{color:#cc9393}.diff .hljs-addition,.hljs-comment,.hljs-doctype,.hljs-pi,.hljs-shebang,.hljs-template_comment,.java .hljs-annotation{color:#7f9f7f}.coffeescript .javascript,.javascript .xml,.tex .hljs-formula,.xml .css,.xml .hljs-cdata,.xml .javascript,.xml .vbscript{opacity:.5}</style></head><body><div id="react-mount"><div data-reactroot="" data-reactid="1" data-react-checksum="-1003748398"><div style="margin-bottom:1.5rem;" class="headroom-wrapper" data-reactid="2"><div style="position:relative;top:0;left:0;right:0;z-index:1;-webkit-transform:translate3D(0, 0, 0);-ms-transform:translate3D(0, 0, 0);transform:translate3D(0, 0, 0);background:lightgray;" class="headroom headroom--unfixed" data-reactid="3"><div style="max-width:960px;margin-left:auto;margin-right:auto;padding-top:0;padding:1.5rem 0.75rem;" data-reactid="4"><a style="color:black;text-decoration:none;" href="/" data-reactid="5">nabilhassein.github.io</a><span style="display:block;clear:both;" data-reactid="6"> </span></div><div style="max-width:960px;margin-left:auto;margin-right:auto;padding-top:0;padding:1.5rem 0.75rem;" data-reactid="7"><a style="color:black;text-decoration:none;" href="/about" data-reactid="8">About</a><span data-reactid="9"> | </span><a style="color:black;text-decoration:none;" href="/projects" data-reactid="10">Projects</a><span data-reactid="11"> | </span><a style="color:black;text-decoration:none;" href="/presentations" data-reactid="12">Presentations</a><span data-reactid="13"> | </span><a style="color:black;text-decoration:none;" href="/resume.pdf" data-reactid="14">Resume</a><span data-reactid="15"> | </span><a style="color:black;text-decoration:none;" href="/contact" data-reactid="16">Contact</a><span style="display:block;clear:both;" data-reactid="17"> </span></div></div></div><div style="max-width:960px;margin-left:auto;margin-right:auto;padding:1.5rem 0.75rem;padding-top:0;" data-reactid="18"><div class="markdown" data-reactid="19"><h1 data-reactid="20">Against Black Inclusion in Facial Recognition</h1><div data-reactid="21"><p><em>This piece was originally published on <a href="https://digitaltalkingdrum.com/2017/08/15/against-black-inclusion-in-facial-recognition/">Digital Talking Drum</a>, formerly known as Decolonized Tech.</em></p>
<p>Researchers have documented the frequent inability of facial recognition software to detect Black faces due to programmers’ use of unrepresentative data to train machine learning models.<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> This issue is not unique, but systemic; in a related example, automated passport photo validation has registered Asian people’s open eyes as being closed.<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> Such technological biases have precedents in mediums older than software. For example, color photography was initially optimized for lighter skin tones at the expense of people with darker skin, a bias corrected mainly due to the efforts and funding of furniture manufacturers and chocolate sellers to render darker tones more easily visible in photographs – the better to sell their products.<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> Groups such as the Algorithmic Justice League have made it their mission to “highlight algorithmic bias” and “develop practices for accountability during the design, development, and deployment of coded systems”.<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> I support all of those goals abstractly, but at a concrete level, I question whose interests would truly be served by the deployment of automated systems capable of reliably identifying Black people.</p>
<p>As a longtime programmer, I know first-hand that software can be an unwelcoming medium for Black folks, not only because of racism among programmers, but also because of biases built into code, which programmers can hardly avoid as no other foundations exist to build on. It’s easy for me to understand a desire to rid software of these biases. Just last month, I wrote up a sketch of a proposal to decolonize the Pronouncing<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> software library I used in a simple art project to generate rhymes modeled on those of my favorite rapper.<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup> Given this and other experiences, I empathized when I heard Joy Buolamwini of the Algorithmic Justice League speak on wearing a white mask to get her own highly imaginative “Aspire Mirror” project involving facial recognition to perceive her existence.<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup> Modern technology has rendered literal Frantz Fanon’s metaphor of “Black Skin, White Masks”.<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup></p>
<p>Facial recognition has diverse applications, but as a police and prison abolitionist, the enhancement of state-controlled surveillance cameras (including police body cameras) to automatically identify people looms much larger in my mind than any other use.<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> Researchers at Georgetown University found that fully half of American adults, or over 100 million people, are registered in one or another law enforcement facial recognition database, drawing from sources such as driver’s license photos.<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> Baltimore Police used the technology to identify participants in the uprising following the murder of Freddie Gray.<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup> The US government plans to use facial recognition to identify every airline passenger exiting the United States.<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup> Machine learning researchers have even reinvented the racist pseudoscience of physiognomy, in a study claiming to identify criminals with approximately 90% accuracy based on their faces alone – using data provided by police.<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></p>
<p>I consider it obvious that most if not all data collected by police to serve their inherently racist mission will be severely biased. It is equally clear to me that no technology under police control will be used to hold police accountable or to benefit Black folks or other oppressed people. Even restricting our attention to machine learning in the so-called “justice” system, examples abound of technology used to harm us, such as racist predictive models used by the courts to determine bail and sentencing decisions – matters of freedom and captivity, life and death.<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> Accordingly, I have no reason to support the development or deployment of technology which makes it easier for the state to recognize and surveil members of my community. Just the opposite: by refusing to don white masks, we may be able to gain some temporary advantages by partially obscuring ourselves from the eyes of white supremacist state. The reality for the foreseeable future is that the people who control and deploy facial recognition technology at any consequential scale will predominantly be our oppressors. Why should we desire our faces to be legible for efficient automated processing by systems of their design? We could demand instead that police be forbidden to use such unreliable surveillance technologies. Anti-racist technologists could engage in high-tech direct action by using the limited resources at our disposal to further develop extant techniques for tricking machine learning models into misclassifications,<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> or distributing anti-surveillance hardware such as glasses designed to obscure the wearer’s face from cameras.<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup></p>
<p>This analysis clearly contradicts advocacy of “diversity and inclusion” as the universal or even typical response to bias. Among the political class, “Black faces in high places” have utterly failed to produce gains for the Black masses.<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> Similarly, Black cops have shown themselves just as likely as white cops to engage in racist brutality and murder.<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> Why should the inclusion of Black folks in facial recognition, or for that matter, the racist technology industry be different? Systemic oppression cannot be addressed by a change in the complexion of the oppressor, as though a rainbow 1% and more white people crowding the prisons would mean justice. That’s not the world I want to live in. We must imagine and build a future of real freedom.</p>
<p>All of the arguments I’ve presented could be (and have been) applied to many domains beyond facial recognition. I continue to grapple with what that means for my own work as a technologist and a political organizer, but I am firm already in at least two conclusions. The first is that despite every disadvantage, we must reappropriate oppressive technology for emancipatory purposes. The second is that the liberation of Black folks and all oppressed peoples will never be achieved by inclusion in systems controlled by a capitalist elite which benefits from the perpetuation of racism and related oppressions. It can only be achieved by the destruction of those systems, and the construction of new technologies designed, developed, and deployed by our own communities for our own benefit. The struggle for liberation is not a struggle for diversity and inclusion – it is a struggle for decolonization, reparations, and self-determination. We can realize those aspirations only in a socialist world.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p><a href="https://www.digitaltrends.com/photography/google-apologizes-for-misidentifying-a-black-couple-as-gorillas-in-photos-app/">https://www.digitaltrends.com/photography/google-apologizes-for-misidentifying-a-black-couple-as-gorillas-in-photos-app/</a>; <a href="https://www.theguardian.com/technology/2017/may/28/joy-buolamwini-when-algorithms-are-racist-facial-recognition-bias">https://www.theguardian.com/technology/2017/may/28/joy-buolamwini-when-algorithms-are-racist-facial-recognition-bias</a> <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p><a href="https://www.dailydot.com/irl/richard-lee-eyes-closed-facial-recognition/">https://www.dailydot.com/irl/richard-lee-eyes-closed-facial-recognition/</a> <a href="#fnref2" class="footnote-backref">↩</a></p>
</li>
<li id="fn3"  class="footnote-item"><p><a href="https://petapixel.com/2015/09/19/heres-a-look-at-how-color-film-was-originally-biased-toward-white-people/">https://petapixel.com/2015/09/19/heres-a-look-at-how-color-film-was-originally-biased-toward-white-people/</a> <a href="#fnref3" class="footnote-backref">↩</a></p>
</li>
<li id="fn4"  class="footnote-item"><p><a href="https://www.ajlunited.org">https://www.ajlunited.org</a> <a href="#fnref4" class="footnote-backref">↩</a></p>
</li>
<li id="fn5"  class="footnote-item"><p><a href="https://pronouncing.readthedocs.io/en/latest/">https://pronouncing.readthedocs.io/en/latest/</a> <a href="#fnref5" class="footnote-backref">↩</a></p>
</li>
<li id="fn6"  class="footnote-item"><p><a href="https://nabilhassein.github.io/blog/generative-doom/">https://nabilhassein.github.io/blog/generative-doom/</a> <a href="#fnref6" class="footnote-backref">↩</a></p>
</li>
<li id="fn7"  class="footnote-item"><p><a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms/">https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms/</a> <a href="#fnref7" class="footnote-backref">↩</a></p>
</li>
<li id="fn8"  class="footnote-item"><p>Frantz Fanon: “Black Skin, White Masks”. <a href="#fnref8" class="footnote-backref">↩</a></p>
</li>
<li id="fn9"  class="footnote-item"><p><a href="https://theintercept.com/2017/03/22/real-time-face-recognition-threatens-to-turn-cops-body-cameras-into-surveillance-machines/">https://theintercept.com/2017/03/22/real-time-face-recognition-threatens-to-turn-cops-body-cameras-into-surveillance-machines/</a> <a href="#fnref9" class="footnote-backref">↩</a></p>
</li>
<li id="fn10"  class="footnote-item"><p><a href="https://www.law.georgetown.edu/news/press-releases/half-of-all-american-adults-are-in-a-police-face-recognition-database-new-report-finds.cfm">https://www.law.georgetown.edu/news/press-releases/half-of-all-american-adults-are-in-a-police-face-recognition-database-new-report-finds.cfm</a> <a href="#fnref10" class="footnote-backref">↩</a></p>
</li>
<li id="fn11"  class="footnote-item"><p><a href="http://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf">http://www.aclunc.org/docs/20161011_geofeedia_baltimore_case_study.pdf</a> <a href="#fnref11" class="footnote-backref">↩</a></p>
</li>
<li id="fn12"  class="footnote-item"><p><a href="https://www.dhs.gov/sites/default/files/publications/privacy-pia-cbp030-tvs-may2017.pdf">https://www.dhs.gov/sites/default/files/publications/privacy-pia-cbp030-tvs-may2017.pdf</a> <a href="#fnref12" class="footnote-backref">↩</a></p>
</li>
<li id="fn13"  class="footnote-item"><p><a href="https://www.rt.com/news/368307-facial-recognition-criminal-china/">https://www.rt.com/news/368307-facial-recognition-criminal-china/</a> <a href="#fnref13" class="footnote-backref">↩</a></p>
</li>
<li id="fn14"  class="footnote-item"><p><a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a> <a href="#fnref14" class="footnote-backref">↩</a></p>
</li>
<li id="fn15"  class="footnote-item"><p><a href="https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture">https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture</a>; <a href="https://cvdazzle.com/">https://cvdazzle.com/</a> <a href="#fnref15" class="footnote-backref">↩</a></p>
</li>
<li id="fn16"  class="footnote-item"><p><a href="https://blogs.wsj.com/japanrealtime/2015/08/07/eyeglasses-with-face-un-recognition-function-to-debut-in-japan/">https://blogs.wsj.com/japanrealtime/2015/08/07/eyeglasses-with-face-un-recognition-function-to-debut-in-japan/</a> <a href="#fnref16" class="footnote-backref">↩</a></p>
</li>
<li id="fn17"  class="footnote-item"><p>Keeanga-Yamahtta Taylor: “From #BlackLivesMatter to Black Liberation”, Chapter 3, “Black Faces in High Places”. <a href="#fnref17" class="footnote-backref">↩</a></p>
</li>
<li id="fn18"  class="footnote-item"><p><a href="https://mic.com/articles/118290/it-s-time-to-talk-about-the-black-police-officers-who-killed-freddie-gray">https://mic.com/articles/118290/it-s-time-to-talk-about-the-black-police-officers-who-killed-freddie-gray</a> <a href="#fnref18" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
</div></div><span style="display:block;clear:both;" data-reactid="22"> </span></div></div></div><script src="/bundle.js?t=1567533331174"></script></body></html>